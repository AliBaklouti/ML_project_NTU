{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":47790,"databundleVersionId":5172264,"sourceType":"competition"},{"sourceId":4449018,"sourceType":"datasetVersion","datasetId":2605336}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part 1: EDA and feature selection\n## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-16T04:06:14.470333Z","iopub.execute_input":"2024-02-16T04:06:14.470780Z","iopub.status.idle":"2024-02-16T04:06:15.698309Z","shell.execute_reply.started":"2024-02-16T04:06:14.470736Z","shell.execute_reply":"2024-02-16T04:06:15.697258Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"execution":{"iopub.status.busy":"2024-02-16T04:06:15.700797Z","iopub.execute_input":"2024-02-16T04:06:15.701757Z","iopub.status.idle":"2024-02-16T04:06:16.802942Z","shell.execute_reply.started":"2024-02-16T04:06:15.701709Z","shell.execute_reply":"2024-02-16T04:06:16.801614Z"},"trusted":true},"outputs":[{"name":"stdout","text":"media-campaign-cost-prediction\tplayground-series-s3e11\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T04:06:16.805578Z","iopub.execute_input":"2024-02-16T04:06:16.806651Z","iopub.status.idle":"2024-02-16T04:06:16.812605Z","shell.execute_reply.started":"2024-02-16T04:06:16.806601Z","shell.execute_reply":"2024-02-16T04:06:16.811332Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Data ingest","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s3e11/train.csv', index_col='id')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e11/test.csv', index_col='id')\noriginal_df = pd.read_csv('/kaggle/input/media-campaign-cost-prediction/train_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-16T04:06:16.815022Z","iopub.execute_input":"2024-02-16T04:06:16.815648Z","iopub.status.idle":"2024-02-16T04:06:18.697066Z","shell.execute_reply.started":"2024-02-16T04:06:16.815618Z","shell.execute_reply":"2024-02-16T04:06:18.695899Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"(train_df.columns == original_df.columns).all()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T04:06:18.698557Z","iopub.execute_input":"2024-02-16T04:06:18.700094Z","iopub.status.idle":"2024-02-16T04:06:18.710006Z","shell.execute_reply.started":"2024-02-16T04:06:18.700061Z","shell.execute_reply":"2024-02-16T04:06:18.708697Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"Train has a \"cost\" column that is our y-values, test does not (we have to predict the cost). \"Original\" is much smaller than train, which was generated from it, but is the \"real\" ground truth.","metadata":{}},{"cell_type":"code","source":"train_df['log_cost'] = np.log(train_df['cost'])\noriginal_df['log_cost'] = np.log(original_df['cost'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We want to fit the log_cost, not the cost, as the kaggle accuracy is MSE of log cost.","metadata":{}},{"cell_type":"markdown","source":"## Initial EDA","metadata":{}},{"cell_type":"code","source":"original_df.nunique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So we actually have only two numerical features, store_sales and gross_weight. The rest are ordinal or categorical.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train_df.corr().abs())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looks like \"salad_bar\" and \"prepared_food\" have a very high correlation with each other -- could be merged into one variable. They're both boolean, so we could just take the mean of the two columns. But is this a good idea? Multicollinearity is not actually bad for many models. \n\nTo a lesser degree, store_sales and unit_sales, and num_children_at_home and total_children, and the various boolean features (coffee_bar, video_store, salar_bar, prepared_food, and florist) form correlated chunks. If the multicollinearity does prove to be an issue, could use PCA to get rid of it -- PCA produces uncorrelated variables.","metadata":{}},{"cell_type":"markdown","source":"## Feature importances attempt #1","metadata":{}},{"cell_type":"markdown","source":"scikit-learn docs suggest using a Random Forest to make an initial guess at the feature importances. Let's try that, on the original_df dataset as the generated dataset (train_df) is too large for a quick fit. Note that this is without using a test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(random_state=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forest.fit(X=original_df.iloc[:(num:=None)].drop(['cost', 'log_cost'], axis=1), y=original_df.iloc[:num]['log_cost'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importances = pd.concat((pd.Series(forest.feature_names_in_), pd.Series(forest.feature_importances_)), axis=1)\nfeature_importances.columns = ['feature', 'importance']\nsns.barplot(feature_importances, x='feature', y='importance')\nplt.xticks(rotation=90)\nNone","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hmmm... The \"most importance features\" look decidedly different from the ones in the top-scoring notebook.","metadata":{}},{"cell_type":"code","source":"#most_important_features = ['total_children', 'num_children_at_home',\n#                           'avg_cars_at home(approx).1', 'store_sqft',\n#                           'coffee_bar', 'video_store', 'salad', \n#                           'florist']\n#and also: store_sales(in millions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Why the discrepancy? I already know the standardisation isn't important for a random forest, at least. Maybe we can try investigating this in more detail. Perhaps the multicollinearity sabotages the feature importances. Seems like it is important to select features by contribution when used with a test set/cross-validation, rather than just by training set feature importances? But a discussion post suggests computing the mutual information and notes that works well as well.","metadata":{}},{"cell_type":"markdown","source":"## Compute the mutual information of each feature with the target","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['salad_food'] = train_df['salad_bar'] + train_df['prepared_food']\ntest_df['salad_food'] = test_df['salad_bar'] + test_df['prepared_food']\noriginal_df['salad_food'] = original_df['salad_bar'] + original_df['prepared_food']\n\ntrain_df = train_df.drop(['salad_bar', 'prepared_food'], axis=1)\ntest_df = test_df.drop(['salad_bar', 'prepared_food'], axis=1)\noriginal_df = original_df.drop(['salad_bar', 'prepared_food'], axis=1)\nNone","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mi_reg = mutual_info_regression(original_df.drop(['log_cost', 'cost'], axis=1), original_df['cost'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mis = pd.DataFrame()\nmis['feature'] = original_df.drop(['log_cost', 'cost'], axis=1).columns\nmis['mi'] = mi_reg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.barplot(mis, x='feature', y='mi')\nplt.xticks(rotation=90)\nNone","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_mi_features = [\n    'unit_sales(in_millions)',\n    'total_children',\n    'num_children_at_home',\n    'avg_cars_at home(approx).1',\n    'store_sqft',\n    'coffee_bar',\n    'video_store',\n    'salad_food',\n    'florist',\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"That looks more like our correct feature importances.","metadata":{}},{"cell_type":"markdown","source":"## Plot the distribution of each feature","metadata":{"execution":{"iopub.status.busy":"2024-01-31T07:55:20.004579Z","iopub.execute_input":"2024-01-31T07:55:20.00501Z","iopub.status.idle":"2024-01-31T07:55:20.011768Z","shell.execute_reply.started":"2024-01-31T07:55:20.004981Z","shell.execute_reply":"2024-01-31T07:55:20.010792Z"}}},{"cell_type":"code","source":"fig, axes = plt.subplots(4, 4, figsize=(5.6*5, 4.8*4))\nfor ax, column in zip(axes.flatten(), list(original_df.columns)):\n    sns.histplot(original_df[column], bins=64, ax=ax)\n    ax.set_title(column)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So, that gives us a neat hypothesis for why LOFO maybe fails on gross_weight.","metadata":{}},{"cell_type":"markdown","source":"## Perform further feature pruning using LOFO and cross-validation","metadata":{}},{"cell_type":"code","source":"lofo_scores = mis.drop('mi', axis=1).copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forest = RandomForestRegressor(random_state=0)\nfor idx, row in tqdm(lofo_scores.iterrows(), total=len(lofo_scores)):\n    lofo_scores.at[idx, 'cv_score_without'] = cross_val_score(\n        forest, \n        original_df.drop([row['feature'], 'log_cost', 'cost'], axis=1),\n        original_df['log_cost'],\n        cv=5,\n        n_jobs=8).mean()\n    print(idx/len(lofo_scores*100))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline = cross_val_score(\n    forest, \n    original_df.drop([row['feature'], 'log_cost', 'cost'], axis=1),\n    original_df['log_cost'],\n    cv=5,\n    n_jobs=8).mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lofo_scores['lofo_score'] = lofo_scores['cv_score_without'] - baseline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.barplot(lofo_scores, x='feature', y='cv_score_without')\nplt.xticks(rotation=90)\nNone","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hmmm... Honestly I'm inclined to just go by the MIs.","metadata":{}},{"cell_type":"markdown","source":"## Analyse duplicates","metadata":{}},{"cell_type":"markdown","source":"Our chosen features are 8 categorical features and maybe one numerical feature. Leaving the numerical feature out, we're left with ~3k groups, instead of 50k or 600k. So, we can greatly accelerate our model training, for our convenience.","metadata":{}},{"cell_type":"code","source":"high_mi_features = [\n    'unit_sales(in millions)',\n    'total_children',\n    'num_children_at_home',\n    'avg_cars_at home(approx).1',\n    'store_sqft',\n    'coffee_bar',\n    'video_store',\n    'salad_food',\n    'florist',\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[high_mi_features].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[high_mi_features].drop('unit_sales(in millions)', axis=1).value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.concat((train_df, original_df))[high_mi_features].drop('unit_sales(in millions)', axis=1).value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Just submit something using a naive model","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s3e11/train.csv', index_col='id')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e11/test.csv', index_col='id')\noriginal_df = pd.read_csv('/kaggle/input/media-campaign-cost-prediction/train_dataset.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['log_cost'] = np.log(train_df['cost'])\noriginal_df['log_cost'] = np.log(original_df['cost'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['salad_food'] = train_df['salad_bar'] + train_df['prepared_food']\ntest_df['salad_food'] = test_df['salad_bar'] + test_df['prepared_food']\noriginal_df['salad_food'] = original_df['salad_bar'] + original_df['prepared_food']\n\ntrain_df = train_df.drop(['salad_bar', 'prepared_food'], axis=1)\ntest_df = test_df.drop(['salad_bar', 'prepared_food'], axis=1)\noriginal_df = original_df.drop(['salad_bar', 'prepared_food'], axis=1)\nNone","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_mi_features = [\n    'unit_sales(in millions)',\n    'total_children',\n    'num_children_at_home',\n    'avg_cars_at home(approx).1',\n    'store_sqft',\n    'coffee_bar',\n    'video_store',\n    'salad_food',\n    'florist',\n]\nhigh_mi_cat_features = high_mi_features[1:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_original_df = pd.concat((train_df, original_df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_original_df_dedup = train_and_original_df.groupby(high_mi_features)['log_cost']\\\n    .agg(['mean', 'count']).sort_values('count', ascending=False).reset_index()\\\n    .rename(columns={'mean': 'log_cost'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_original_df_dedup","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgbr = XGBRegressor()\nxgbr.fit(X=train_and_original_df_dedup[high_mi_cat_features],\n          y=train_and_original_df_dedup['log_cost'],\n          sample_weight=train_and_original_df_dedup['count'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['cost'] = np.expm1(xgbr.predict(test_df[high_mi_cat_features]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['cost'].to_csv('/kaggle/working/predict.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Wooooo! 284/953!","metadata":{}},{"cell_type":"markdown","source":"# Part 3: Use grid-search CV to produce an optimised XGBoost model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom xgboost import XGBRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params = {'learning_rate': 0.05,\n              'tree_method': 'hist',\n              'enable_categorical': True,\n              'verbosity': 1,\n              'random_state': 1,\n              'eval_metric': 'rmse'}\nxgb_tuned_params = {\n    'n_estimators': np.linspace(20, 1000, 100).astype(int),\n    'max_depth': np.linspace(1, 20, 100).astype(int),\n    'min_child_weight': np.linspace(0, 5, 100).astype(int),\n}\nxgbr = XGBRegressor(**xgb_params)\ncv = RandomizedSearchCV(xgbr,\n                        xgb_tuned_params,\n                        n_jobs=8,\n                        n_iter=20,\n                        verbose=3,\n                        scoring='neg_root_mean_squared_error')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv.fit(X=train_and_original_df_dedup[high_mi_cat_features],\n       y=train_and_original_df_dedup['log_cost'],\n       sample_weight=train_and_original_df_dedup['count'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['cost'] = np.expm1(cv.best_estimator_.predict(test_df[high_mi_cat_features]))\ntest_df['cost'].to_csv('/kaggle/working/predict_hyperparams.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv.best_score_, cv.best_params_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"214/953!","metadata":{}},{"cell_type":"markdown","source":"## Part 4: Try a LightGBM regressor instead","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_params = {'learning_rate': 0.1,\n               'tree_method': 'hist',\n               'random_state': 1,\n               'eval_metric': 'rmse',\n               'categorical_feature': [high_mi_cat_features.index('store_sqft')],\n               'verbose': -1}\nlgbm_tuned_params = {\n    'n_estimators': np.linspace(20, 1000, 100).astype(int),\n    'num_leaves': np.linspace(20, 1000, 100).astype(int),\n    'min_child_weight': np.linspace(0, 5, 100).astype(int),\n}\nlgbmr = LGBMRegressor(**lgbm_params)\ncv_lgbm = RandomizedSearchCV(lgbmr,\n                             lgbm_tuned_params,\n                             n_jobs=8,\n                             n_iter=20,\n                             verbose=3,\n                             scoring='neg_root_mean_squared_error')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_lgbm.fit(X=train_and_original_df_dedup[high_mi_cat_features],\n       y=train_and_original_df_dedup['log_cost'],\n       sample_weight=train_and_original_df_dedup['count'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['cost'] = np.expm1(cv_lgbm.best_estimator_.predict(test_df[high_mi_cat_features]))\ntest_df['cost'].to_csv('/kaggle/working/predict_lgbm.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cv_lgbm.best_score_, cv_lgbm.best_params_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Position: 225/953... What about an ensemble?","metadata":{}},{"cell_type":"markdown","source":"## Part 5: Try an ensemble?","metadata":{}},{"cell_type":"code","source":"train_and_original_df_dedup['xgb_pred_log_cost'] = cv.best_estimator_.predict(train_and_original_df_dedup[high_mi_cat_features])\ntrain_and_original_df_dedup['lgbm_pred_log_cost'] = cv_lgbm.best_estimator_.predict(train_and_original_df_dedup[high_mi_cat_features])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Ridge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rr = Ridge()\nrr.fit(X=train_and_original_df_dedup[['xgb_pred_log_cost', 'lgbm_pred_log_cost']],\n       y=train_and_original_df_dedup['log_cost'],\n       sample_weight=train_and_original_df_dedup['count'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['xgb_pred_log_cost'] = cv.best_estimator_.predict(test_df[high_mi_cat_features])\ntest_df['lgbm_pred_log_cost'] = cv_lgbm.best_estimator_.predict(test_df[high_mi_cat_features])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test_df['cost'] = np.expm1(rr.predict(test_df[['xgb_pred_log_cost', 'lgbm_pred_log_cost']]))\ntest_df['cost'] = np.expm1(np.mean(test_df[['xgb_pred_log_cost', 'lgbm_pred_log_cost']], axis=1))\ntest_df['cost'].to_csv('/kaggle/working/predict_ensemble_unweighted.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hmmm... Looks like ensembling with the LightGBM, at least, worsens performance on test...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}